{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420c3e04",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "1. Build a fully connected neural network for the housing dataset you did in previous homework. For training and validation use 80% (training) and 20% (validation) split. For this part, only use one hidden layer with 8 nodes. Train your network for 200 epochs. Report your training time, training loss, and evaluation accuracy after 300 epochs. Analyze your results in your report. Make sure to submit your code by providing the GitHub URL of your course repository for this course. (15pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3b6044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.indexers import validate_indices\n",
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import housing data\n",
    "housing = pd.DataFrame(pd.read_csv(\".\\TestData\\Housing.csv\"))\n",
    "\n",
    "# Housing input data\n",
    "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking'] \n",
    "price_labels = housing['price']\n",
    "\n",
    "# Get num of housing samples\n",
    "n_samples = housing.shape[0]\n",
    "\n",
    "# 80/20 split for data\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "# Make a random shuffle of the housing indicies\n",
    "housing_shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "# 80% training indicies/ 20% validation indicies\n",
    "train_indices = housing_shuffled_indices[:-n_val]\n",
    "val_indices = housing_shuffled_indices[-n_val:]\n",
    "\n",
    "# Create a tensor holding the input variables columns\n",
    "input_vars = torch.tensor(housing[num_vars].values).float()\n",
    "train_t =  0.00001 * input_vars[train_indices]\n",
    "val_t =  0.001 * input_vars[val_indices]\n",
    "\n",
    "prices = torch.tensor(price_labels.values).float()\n",
    "train_prices = 0.000001 * prices[train_indices]\n",
    "val_prices =  0.000001 * prices[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "884b68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(5,8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8,1))\n",
    "\n",
    "# Use SGD optimizer wiht a learning rate 1e-3\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training Loop iterates through n epochs and returns parameters\n",
    "def training_loop(n_epochs, optimizer, seq_model, loss_fn, training_data, val_data, training_p, val_p):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Assigned predicted outputs to outputs with vailidation data\n",
    "        outputs = seq_model(val_data)\n",
    "        \n",
    "        # Using the loss function, calculate outputs to validation data\n",
    "        val_loss = loss_fn(outputs, val_p)\n",
    "        \n",
    "        # Using the loss function, calculate outputs to training data\n",
    "        outputs = seq_model(training_data)\n",
    "        train_loss = loss_fn(outputs, training_p)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch % 20 == 0:\n",
    "            print(\"Epoch: %d, Training Loss: %f, Validation Loss %f \"% (epoch, float(train_loss), float(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff07312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 27.238184, Validation Loss 26.520161 \n",
      "Epoch: 20, Training Loss: 24.114426, Validation Loss 23.708412 \n",
      "Epoch: 40, Training Loss: 21.186956, Validation Loss 21.148129 \n",
      "Epoch: 60, Training Loss: 18.532696, Validation Loss 18.914503 \n",
      "Epoch: 80, Training Loss: 16.112673, Validation Loss 16.953829 \n",
      "Epoch: 100, Training Loss: 13.922760, Validation Loss 15.231421 \n",
      "Epoch: 120, Training Loss: 11.973709, Validation Loss 13.723233 \n",
      "Epoch: 140, Training Loss: 10.276603, Validation Loss 12.410438 \n",
      "Epoch: 160, Training Loss: 8.834162, Validation Loss 11.276230 \n",
      "Epoch: 180, Training Loss: 7.637640, Validation Loss 10.304242 \n",
      "Epoch: 200, Training Loss: 6.667828, Validation Loss 9.477973 \n",
      "Epoch: 220, Training Loss: 5.898288, Validation Loss 8.780815 \n",
      "Epoch: 240, Training Loss: 5.299153, Validation Loss 8.196451 \n",
      "Epoch: 260, Training Loss: 4.840415, Validation Loss 7.709284 \n",
      "Epoch: 280, Training Loss: 4.494230, Validation Loss 7.304871 \n",
      "Epoch: 300, Training Loss: 4.236217, Validation Loss 6.970190 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\destr\\anaconda3\\envs\\RLMachineLearning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([109])) that is different to the input size (torch.Size([109, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\destr\\anaconda3\\envs\\RLMachineLearning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([436])) that is different to the input size (torch.Size([436, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "training_loop(300, optimizer, seq_model, nn.MSELoss(), train_t, val_t, train_prices, val_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c63750",
   "metadata": {},
   "source": [
    "2. Extend your network with two more additional hidden layers, like the example we did in lecture. Train your network for 300 epochs. Report your training time, training loss, and evaluation accuracy after 300 epochs. Analyze your results in your report. Make sure to submit your code by providing the GitHub URL of your course repository for this course. Analyze your results in your report and compare your model size and accuracy over the baseline implementation in Problem1. a. Do you see any over-fitting? Make sure to submit your code by providing the GitHub URL of your course repository for this course. (25pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "255047f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\destr\\anaconda3\\envs\\RLMachineLearning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([109])) that is different to the input size (torch.Size([109, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\destr\\anaconda3\\envs\\RLMachineLearning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([436])) that is different to the input size (torch.Size([436, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 24.778212, Validation Loss 24.951902 \n",
      "Epoch: 20, Training Loss: 20.615866, Validation Loss 20.789732 \n",
      "Epoch: 40, Training Loss: 16.768064, Validation Loss 16.928698 \n",
      "Epoch: 60, Training Loss: 13.387086, Validation Loss 13.507648 \n",
      "Epoch: 80, Training Loss: 10.517336, Validation Loss 10.579383 \n",
      "Epoch: 100, Training Loss: 8.222343, Validation Loss 8.227841 \n",
      "Epoch: 120, Training Loss: 6.513674, Validation Loss 6.482130 \n",
      "Epoch: 140, Training Loss: 5.330537, Validation Loss 5.286778 \n",
      "Epoch: 160, Training Loss: 4.563473, Validation Loss 4.527309 \n",
      "Epoch: 180, Training Loss: 4.092894, Validation Loss 4.075519 \n",
      "Epoch: 200, Training Loss: 3.816616, Validation Loss 3.821890 \n",
      "Epoch: 220, Training Loss: 3.659793, Validation Loss 3.687015 \n",
      "Epoch: 240, Training Loss: 3.573000, Validation Loss 3.619301 \n",
      "Epoch: 260, Training Loss: 3.525854, Validation Loss 3.587719 \n",
      "Epoch: 280, Training Loss: 3.500593, Validation Loss 3.574654 \n",
      "Epoch: 300, Training Loss: 3.487193, Validation Loss 3.570562 \n"
     ]
    }
   ],
   "source": [
    "# Model Definition with two additional hidden layers\n",
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(5,8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8,3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(3,18),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(18,1))\n",
    "\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(300, optimizer, seq_model, nn.MSELoss(), train_t, val_t, train_prices, val_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc67490",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "1. Create a fully connected Neural Network for all 10 classes in CIFAR-10 with only one hidden layer with the size of 512. Train your network for 200 epochs. Report your training time, training loss and evaluation accuracy after 300 epochs. Analyze your results in your report. Make sure to submit your code by providing the GitHub URL of your course repository for this course. (25pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34aa1178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "from torchvision import transforms\n",
    "\n",
    "# Preprocess images\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )])\n",
    "\n",
    "# Store validation and training data, preprocess data\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform = preprocess)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform = preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e4445e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle = False)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(3072, 512), \n",
    "                      nn.Tanh(), \n",
    "                      nn.Linear(512,10), \n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 300\n",
    "\n",
    "def training_loop(n_epochs, optimizer, loss_fn, train_loader, val_loader):\n",
    "# Iterates through each epoch\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "    # Calculate training loss\n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = model(imgs.view(batch_size, -1))\n",
    "            train_loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    # Zeros gradient for backpropagation and gradient calculation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate validation loss and accuracy\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for imgs, labels in val_loader:\n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = model(imgs.view(batch_size, -1))\n",
    "            _, label_p = torch.max(outputs, dim=1)\n",
    "            count += labels.shape[0]\n",
    "            correct += int((label_p == labels).sum())\n",
    "            val_loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "        acc = correct/count\n",
    "        \n",
    "        if epoch == 1 or epoch % 20 == 0:\n",
    "            print(\"Epoch: %d, Training Loss: %f, Validation Loss %f, Accuracy %f \"% (epoch, float(train_loss), float(val_loss), acc))\n",
    "\n",
    "    \n",
    "\n",
    "#def validate(model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3d8846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 2.503679, Validation Loss 2.300290, Accuracy 0.091500 \n",
      "Epoch: 20, Training Loss: 2.348368, Validation Loss 2.024601, Accuracy 0.201200 \n",
      "Epoch: 40, Training Loss: 2.239162, Validation Loss 1.792625, Accuracy 0.258600 \n",
      "Epoch: 60, Training Loss: 2.120733, Validation Loss 2.002821, Accuracy 0.307900 \n",
      "Epoch: 80, Training Loss: 2.046415, Validation Loss 1.961299, Accuracy 0.302800 \n",
      "Epoch: 100, Training Loss: 1.821823, Validation Loss 1.804232, Accuracy 0.309500 \n",
      "Epoch: 120, Training Loss: 1.763888, Validation Loss 1.838506, Accuracy 0.319600 \n",
      "Epoch: 140, Training Loss: 2.195405, Validation Loss 1.958158, Accuracy 0.310500 \n",
      "Epoch: 160, Training Loss: 2.415083, Validation Loss 1.851601, Accuracy 0.327400 \n",
      "Epoch: 180, Training Loss: 1.861718, Validation Loss 1.640934, Accuracy 0.323000 \n",
      "Epoch: 200, Training Loss: 2.435525, Validation Loss 1.651738, Accuracy 0.344600 \n",
      "Epoch: 220, Training Loss: 1.634723, Validation Loss 1.773664, Accuracy 0.320000 \n",
      "Epoch: 240, Training Loss: 2.003531, Validation Loss 1.802948, Accuracy 0.343800 \n",
      "Epoch: 260, Training Loss: 2.049707, Validation Loss 1.932860, Accuracy 0.346400 \n",
      "Epoch: 280, Training Loss: 1.494610, Validation Loss 1.887852, Accuracy 0.330000 \n",
      "Epoch: 300, Training Loss: 1.925499, Validation Loss 1.867260, Accuracy 0.343200 \n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs, optimizer, loss_fn, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb735b0a",
   "metadata": {},
   "source": [
    "2. Extend your network with two more additional hidden layers, like the example we did in lecture. Train your network for 300 epochs. Report your training time, loss, and evaluation accuracy after 300 epochs. Analyze your results in your report and compare your model size and accuracy over the baseline implementation in Problem1. a. Do you see any over-fitting? Make sure to submit your code by providing the GitHub URL of your course repository for this course. (35pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4a24fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 2.297091, Validation Loss 2.274341, Accuracy 0.092600 \n",
      "Epoch: 20, Training Loss: 2.257580, Validation Loss 2.272160, Accuracy 0.134300 \n",
      "Epoch: 40, Training Loss: 2.159376, Validation Loss 2.252708, Accuracy 0.180900 \n",
      "Epoch: 60, Training Loss: 2.251507, Validation Loss 2.226292, Accuracy 0.194900 \n",
      "Epoch: 80, Training Loss: 2.168830, Validation Loss 2.215267, Accuracy 0.198200 \n",
      "Epoch: 100, Training Loss: 2.173899, Validation Loss 2.201686, Accuracy 0.194400 \n",
      "Epoch: 120, Training Loss: 2.205384, Validation Loss 2.166289, Accuracy 0.205300 \n",
      "Epoch: 140, Training Loss: 2.206985, Validation Loss 2.135243, Accuracy 0.218700 \n",
      "Epoch: 160, Training Loss: 2.082662, Validation Loss 2.119309, Accuracy 0.243700 \n",
      "Epoch: 180, Training Loss: 2.158086, Validation Loss 2.123203, Accuracy 0.239400 \n",
      "Epoch: 200, Training Loss: 2.187004, Validation Loss 2.098309, Accuracy 0.241600 \n",
      "Epoch: 220, Training Loss: 2.116856, Validation Loss 2.057478, Accuracy 0.246600 \n",
      "Epoch: 240, Training Loss: 2.071825, Validation Loss 2.054954, Accuracy 0.249400 \n",
      "Epoch: 260, Training Loss: 2.237162, Validation Loss 2.040205, Accuracy 0.248300 \n",
      "Epoch: 280, Training Loss: 2.015700, Validation Loss 2.097916, Accuracy 0.227300 \n",
      "Epoch: 300, Training Loss: 2.097558, Validation Loss 2.088956, Accuracy 0.231600 \n"
     ]
    }
   ],
   "source": [
    "# Model with two addional hidden layers\n",
    "model = nn.Sequential(nn.Linear(3072, 512), \n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 5), \n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(5, 50), \n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(50, 10), \n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "training_loop(n_epochs, optimizer, loss_fn, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dac61e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Tanh is treated as a zero-op.\n",
      "Sequential(\n",
      "  0.0 M, 100.000% Params, 0.0 GMac, 100.000% MACs, \n",
      "  (0): Linear(0.0 M, 84.211% Params, 0.0 GMac, 84.211% MACs, in_features=5, out_features=8, bias=True)\n",
      "  (1): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (2): Linear(0.0 M, 15.789% Params, 0.0 GMac, 15.789% MACs, in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           57      \n",
      "Warning: module Tanh is treated as a zero-op.\n",
      "Sequential(\n",
      "  0.0 M, 100.000% Params, 0.0 GMac, 100.000% MACs, \n",
      "  (0): Linear(0.0 M, 28.916% Params, 0.0 GMac, 28.916% MACs, in_features=5, out_features=8, bias=True)\n",
      "  (1): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (2): Linear(0.0 M, 16.265% Params, 0.0 GMac, 16.265% MACs, in_features=8, out_features=3, bias=True)\n",
      "  (3): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (4): Linear(0.0 M, 43.373% Params, 0.0 GMac, 43.373% MACs, in_features=3, out_features=18, bias=True)\n",
      "  (5): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (6): Linear(0.0 M, 11.446% Params, 0.0 GMac, 11.446% MACs, in_features=18, out_features=1, bias=True)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           166     \n",
      "Warning: module Tanh is treated as a zero-op.\n",
      "Warning: module LogSoftmax is treated as a zero-op.\n",
      "Sequential(\n",
      "  1.579 M, 100.000% Params, 0.002 GMac, 100.000% MACs, \n",
      "  (0): Linear(1.573 M, 99.675% Params, 0.002 GMac, 99.675% MACs, in_features=3072, out_features=512, bias=True)\n",
      "  (1): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (2): Linear(0.005 M, 0.325% Params, 0.0 GMac, 0.325% MACs, in_features=512, out_features=10, bias=True)\n",
      "  (3): LogSoftmax(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, dim=1)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           1.58 M  \n",
      "Warning: module Tanh is treated as a zero-op.\n",
      "Warning: module LogSoftmax is treated as a zero-op.\n",
      "Sequential(\n",
      "  1.577 M, 100.000% Params, 0.002 GMac, 100.000% MACs, \n",
      "  (0): Linear(1.573 M, 99.786% Params, 0.002 GMac, 99.786% MACs, in_features=3072, out_features=512, bias=True)\n",
      "  (1): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (2): Linear(0.003 M, 0.163% Params, 0.0 GMac, 0.163% MACs, in_features=512, out_features=5, bias=True)\n",
      "  (3): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (4): Linear(0.0 M, 0.019% Params, 0.0 GMac, 0.019% MACs, in_features=5, out_features=50, bias=True)\n",
      "  (5): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (6): Linear(0.001 M, 0.032% Params, 0.0 GMac, 0.032% MACs, in_features=50, out_features=10, bias=True)\n",
      "  (7): LogSoftmax(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, dim=1)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           1.58 M  \n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(5,8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8,1))\n",
    "\n",
    "macs, params = get_model_complexity_info(seq_model, (1, 5), as_strings=True,\n",
    "                                                   print_per_layer_stat=True, verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "    \n",
    "seq_model2 = nn.Sequential(\n",
    "        nn.Linear(5,8),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(8,3),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(3,18),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(18,1))\n",
    "    \n",
    "    \n",
    "macs, params = get_model_complexity_info(seq_model2, (1, 5), as_strings=True,\n",
    "                                                   print_per_layer_stat=True, verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "model1 = nn.Sequential(nn.Linear(3072, 512), \n",
    "                      nn.Tanh(), \n",
    "                      nn.Linear(512,10), \n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "macs, params = get_model_complexity_info(model1, (1, 3072), as_strings=True,\n",
    "                                                   print_per_layer_stat=True, verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1, 3072), as_strings=True,\n",
    "                                                   print_per_layer_stat=True, verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96aab7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert[webpdf] in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (6.1.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (3.0.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (0.3)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (2.11.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (4.9.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (0.5.11)\n",
      "Requirement already satisfied: nbformat>=4.4 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (5.1.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (4.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (0.7.1)\n",
      "Requirement already satisfied: testpath in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (0.8.4)\n",
      "Requirement already satisfied: traitlets>=5.0 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbconvert[webpdf]) (5.1.1)\n",
      "Collecting pyppeteer==0.2.2\n",
      "  Downloading pyppeteer-0.2.2-py3-none-any.whl (145 kB)\n",
      "Collecting urllib3<2.0.0,>=1.25.8\n",
      "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Collecting tqdm<5.0.0,>=4.42.1\n",
      "  Downloading tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting websockets<9.0,>=8.1\n",
      "  Downloading websockets-8.1-cp38-cp38-win_amd64.whl (66 kB)\n",
      "Collecting pyee<8.0.0,>=7.0.1\n",
      "  Downloading pyee-7.0.4-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from jinja2>=2.4->nbconvert[webpdf]) (2.0.1)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert[webpdf]) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert[webpdf]) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert[webpdf]) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert[webpdf]) (22.3.0)\n",
      "Requirement already satisfied: tornado>=4.1 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert[webpdf]) (6.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from jupyter-core->nbconvert[webpdf]) (302)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbformat>=4.4->nbconvert[webpdf]) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from nbformat>=4.4->nbconvert[webpdf]) (0.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert[webpdf]) (0.18.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert[webpdf]) (1.16.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert[webpdf]) (58.0.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert[webpdf]) (21.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer==0.2.2->nbconvert[webpdf]) (0.4.4)\n",
      "Requirement already satisfied: webencodings in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from bleach->nbconvert[webpdf]) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from bleach->nbconvert[webpdf]) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from packaging->bleach->nbconvert[webpdf]) (3.0.4)\n",
      "Installing collected packages: websockets, urllib3, tqdm, pyee, appdirs, pyppeteer\n",
      "Successfully installed appdirs-1.4.4 pyee-7.0.4 pyppeteer-0.2.2 tqdm-4.63.0 urllib3-1.26.8 websockets-8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbconvert[webpdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34736c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandoc\n",
      "  Downloading pandoc-2.1.tar.gz (29 kB)\n",
      "Collecting plumbum\n",
      "  Downloading plumbum-1.7.2-py2.py3-none-any.whl (117 kB)\n",
      "Collecting ply\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\destr\\anaconda3\\envs\\rlmachinelearning\\lib\\site-packages (from plumbum->pandoc) (302)\n",
      "Building wheels for collected packages: pandoc\n",
      "  Building wheel for pandoc (setup.py): started\n",
      "  Building wheel for pandoc (setup.py): finished with status 'done'\n",
      "  Created wheel for pandoc: filename=pandoc-2.1-py3-none-any.whl size=29536 sha256=0a3ea30d8ce7c160d0f2a90391189ff5decb74f78f8826e8421b5882485c58de\n",
      "  Stored in directory: c:\\users\\destr\\appdata\\local\\pip\\cache\\wheels\\ce\\41\\63\\bf7cb60c03dc7f93180e91e0972c12345b40bf59212d307157\n",
      "Successfully built pandoc\n",
      "Installing collected packages: ply, plumbum, pandoc\n",
      "Successfully installed pandoc-2.1 plumbum-1.7.2 ply-3.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02104fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
